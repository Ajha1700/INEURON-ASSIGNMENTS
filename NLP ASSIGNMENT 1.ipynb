{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Explain One-Hot Encoding\n",
    "\n",
    "ANSWER\n",
    "\n",
    "One-hot encoding is a technique used in machine learning to represent categorical data numerically.\n",
    "It involves converting each categorical value into a binary vector with a length equal to the number of categories \n",
    "in the variable. Each vector has a 1 in the position corresponding to the category it represents and 0s in all other positions.\n",
    "\n",
    "For example, consider a categorical variable \"color\" with three categories: red, green, and blue. One-hot encoding \n",
    "would create three binary vectors, one for each category:\n",
    "\n",
    "Red: [1, 0, 0]\n",
    "Green: [0, 1, 0]\n",
    "Blue: [0, 0, 1]\n",
    "Each data point in the \"color\" variable would then be represented by one of these vectors. This enables machine\n",
    "learning algorithms to work with categorical data, which would otherwise be difficult to process in their original form.\n",
    "\n",
    "One-hot encoding is commonly used in natural language processing (NLP) to represent words or tokens as vectors, \n",
    "and in recommender systems to represent user preferences.\n",
    "\n",
    "One Hot Encoding is a process by which categorical variables are converted into a form that could be provided to ML \n",
    "algorithms to do a better job in prediction.\n",
    "\n",
    "One hot encoding is the most widespread approach, and it works very well unless your categorical variable\n",
    "takes on a large number of values (i.e. you generally won't it for variables taking more than 15 different values.\n",
    "It'd be a poor choice in some cases with fewer values, though that varies.)\n",
    "\n",
    "One hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.Explain Bag of Words?\n",
    "\n",
    "ANSWER\n",
    "\n",
    "The bag-of-words (BOW) model is a representation that turns arbitrary text into fixed-length vectors by counting how many \n",
    "times each word appears. This process is often referred to as vectorization.\n",
    "\n",
    "Suppose we wanted to vectorize the following:\n",
    "the cat sat\n",
    "the cat sat in the hat\n",
    "the cat with the hat\n",
    "\n",
    "We’ll refer to each of these as a text document.\n",
    "\n",
    "We first define our vocabulary, which is the set of all words found in our document set. The only words that are found \n",
    "in the 3 documents above are: the, cat, sat, in, the, hat, and with.\n",
    "\n",
    "Now we have length-6 vectors for each document!\n",
    "the cat sat: [1, 1, 1, 0, 0, 0]\n",
    "the cat sat in the hat: [2, 1, 1, 1, 1, 0]\n",
    "the cat with the hat: [2, 1, 0, 0, 1, 1]\n",
    "The Problem with BOW is, it does not presever contextual infomration among the words. Notice that we\n",
    "lose contextual information, e.g. where in the document the word appeared, when we use BOW. It’s like a literal bag-of-words:\n",
    "it only tells you what words occur in the document, not where they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a37418",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Explain Bag of N-Grams?\n",
    "\n",
    "ANSWER\n",
    "\n",
    "A Bag-of-N-Grams model is a way to represent a document, similar to a [bag-of-words][/terms/bag-of-words/] model.\n",
    "\n",
    "A bag-of-n-grams model represents a text document as an unordered collection of its n-grams.\n",
    "\n",
    "For example, let’s use the following phrase and divide it into bi-grams (n=2).\n",
    "\n",
    "James is the best person ever.\n",
    "\n",
    "becomes\n",
    "\n",
    "<start>James\n",
    "James is\n",
    "is the\n",
    "the best\n",
    "best person\n",
    "person ever.\n",
    "ever.<end>\n",
    "In a typical bag-of-n-grams model, these 6 bigrams would be a sample from a large number of bigrams observed in a corpus.\n",
    "And then James is the best person ever. would be encoded in a representation showing which of the corpus’s bigrams were\n",
    "observed in the sentence.\n",
    "\n",
    "A bag-of-n-grams model has the simplicity of the bag-of-words model, but allows the preservation of more word \n",
    "locality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2db65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.Explain TF-IDF?\n",
    "\n",
    "ANSWER\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency and is a technique used in Natural Language Processing (NLP) \n",
    "to evaluate the importance of each word or term in a document within a corpus.\n",
    "\n",
    "TF-IDF calculates a score for each word or term that reflects its frequency (TF) within the document and the inverse\n",
    "frequency of the term across all documents in the corpus (IDF).\n",
    "\n",
    "The term frequency (TF) is the number of times a word appears in a document divided by the total number of words in\n",
    "the document. The inverse document frequency (IDF) is the logarithm of the ratio between the total number of documents\n",
    "in the corpus and the number of documents that contain the term. The IDF score is higher for words that appear in fewer\n",
    "documents and lower for words that appear in many documents.\n",
    "\n",
    "The TF-IDF score for a word or term in a document is the product of its TF and IDF scores. Words with higher TF-IDF \n",
    "scores are considered more important in the document, as they are both frequent within the document and rare in the corpus.\n",
    "\n",
    "For example, consider the following two sentences:\n",
    "\n",
    "The cat chased the mouse.\n",
    "The dog chased the cat.\n",
    "The word \"cat\" appears in both sentences but has a higher TF-IDF score in the first sentence because it appears \n",
    "less frequently in the corpus. In contrast, the word \"dog\" has a higher TF-IDF score in the second sentence.\n",
    "\n",
    "TF-IDF is widely used in information retrieval, text classification, and other NLP tasks to identify relevant \n",
    "documents or classify them based on their content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.What is OOV problem?\n",
    "\n",
    "ANSWER\n",
    "\n",
    "The OOV (Out-of-Vocabulary) problem refers to the situation in Natural Language Processing (NLP) where a word\n",
    "or term that is encountered during testing or inference is not present in the vocabulary used during training. \n",
    "This can lead to errors in the model's predictions or inability to process the input correctly.\n",
    "\n",
    "The OOV problem can occur in many NLP tasks, such as text classification, sentiment analysis, and machine\n",
    "translation. It often arises due to the limited size of the vocabulary used during training, which may not\n",
    "include all possible words or variations of words that can appear in the testing or real-world data.\n",
    "\n",
    "There are several techniques used to handle the OOV problem in NLP models, including:\n",
    "\n",
    "Subword tokenization: This involves breaking words into smaller subwords or characters and representing them \n",
    "    as sequences of tokens. This can improve the model's ability to recognize new or unseen words that share\n",
    "    similar subwords or characters.\n",
    "\n",
    "Word embedding models: These models represent words as high-dimensional vectors that capture their semantic meaning and\n",
    "    context. They can handle unseen words by mapping them to the nearest vectors in the embedding space based on their context.\n",
    "\n",
    "Dictionary-based methods: These methods use external resources such as dictionaries or ontologies to handle OOV words \n",
    "    by mapping them to known concepts or similar words.\n",
    "\n",
    "Language modeling: These models learn to predict the probability of the next word given the previous context. \n",
    "    They can be used to generate likely candidates for unseen words based on their context.\n",
    "\n",
    "Handling the OOV problem is an essential aspect of building robust and effective NLP models that can handle a wide range \n",
    "of inputs and scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.What are word embeddings\n",
    "\n",
    "ANSWER\n",
    "\n",
    "Word embeddings are a type of language modeling technique used in Natural Language Processing (NLP) to represent words\n",
    "as numerical vectors in a high-dimensional space. The idea behind word embeddings is to capture the meaning and context \n",
    "of words based on their usage in a large corpus of text data.\n",
    "\n",
    "Word embeddings are learned from the text data using unsupervised machine learning techniques such as neural networks or \n",
    "matrix factorization. The model learns to represent each word as a dense vector of floating-point numbers, where each\n",
    "dimension of the vector corresponds to a particular feature or aspect of the word's meaning.\n",
    "\n",
    "Word embeddings have several advantages over traditional sparse representations of words such as one-hot encoding \n",
    "or bag-of-words. Some of these advantages include:\n",
    "\n",
    "Dimensionality reduction: Word embeddings reduce the high-dimensional space of words to a lower-dimensional space,\n",
    "    making it easier to work with and visualize.\n",
    "\n",
    "Semantic relationships: Word embeddings capture the semantic relationships between words, such as similarity or analogy.\n",
    "    For example, the vectors for \"king\" and \"queen\" will be closer together in the embedding space than \"king\" and \"car.\"\n",
    "\n",
    "Contextual information: Word embeddings capture the context in which words appear in a corpus of text data. \n",
    "    This allows them to capture the nuances and variations in the meaning of words based on their usage in different contexts.\n",
    "\n",
    "Word embeddings are widely used in NLP tasks such as text classification, sentiment analysis, machine translation,\n",
    "and information retrieval. They have proven to be an effective way to represent words and phrases in a compact and \n",
    "meaningful way that captures their semantic and syntactic properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.Explain Continuous bag of words (CBOW)?\n",
    "\n",
    "ANSWER\n",
    "\n",
    "Continuous Bag of Words (CBOW) is a type of neural network architecture used for generating word embeddings in\n",
    "Natural Language Processing (NLP). CBOW is an unsupervised learning algorithm that learns to predict a target word\n",
    "based on the surrounding context words.\n",
    "\n",
    "In the CBOW model, the input layer contains the context words for a target word, represented as one-hot encoded vectors.\n",
    "These context word vectors are then fed into a hidden layer, where their embeddings are learned through the process of training.\n",
    "The output layer of the CBOW model predicts the target word based on the learned embeddings of the context words.\n",
    "\n",
    "The goal of the CBOW model is to minimize the loss function, which measures the difference between the predicted\n",
    "target word and the actual target word. During training, the weights of the CBOW model are updated through backpropagation\n",
    "to minimize the loss function and improve the accuracy of the predictions.\n",
    "\n",
    "The CBOW model has several advantages over other neural network architectures for generating word embeddings\n",
    " Some of these advantages include:\n",
    "\n",
    "1.Efficient training: CBOW is computationally efficient and can be trained on large amounts of data quickly.1\n",
    "\n",
    "2.Robustness: CBOW is less sensitive to noisy data than other models because it averages over the context words,\n",
    "    which helps to reduce the impact of outliers.\n",
    "\n",
    "3.Contextual information: CBOW captures the context in which words appear, which allows it to generate embeddings that \n",
    "    capture the meaning and usage of words based on their context.\n",
    "\n",
    "CBOW is widely used in NLP tasks such as text classification, sentiment analysis, and machine translation, \n",
    "where it is used to generate high-quality word embeddings that capture the semantic and syntactic properties of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.Explain SkipGram?\n",
    "\n",
    "ANSWER\n",
    "\n",
    "SkipGram is a neural network architecture used for generating word embeddings in Natural Language Processing (NLP).\n",
    "Unlike Continuous Bag of Words (CBOW), which predicts a target word based on the surrounding context words, SkipGram\n",
    "predicts the context words based on a given target word.\n",
    "\n",
    "In the SkipGram model, the input layer contains a target word represented as a one-hot encoded vector. The hidden \n",
    "layer of the model learns the embeddings of the target word, which are then used to predict the embeddings of the context words.\n",
    "The output layer of the SkipGram model consists of a softmax function, which produces a probability distribution over the\n",
    "vocabulary of the corpus, with each word assigned a probability based on its co-occurrence frequency with the target word.\n",
    "\n",
    "During training, the SkipGram model adjusts its weights to maximize the probability of the context words given the target word.\n",
    "The goal is to minimize the negative log-likelihood of the observed context words given the target word, which is equivalent to\n",
    "maximizing the average log probability of the context words given the target word.\n",
    "\n",
    "The SkipGram model has several advantages over other neural network architectures for generating word embeddings.\n",
    "Some of these advantages include:\n",
    "\n",
    "1.Capturing rare words: SkipGram can capture rare words and their usage patterns by generating embeddings for infrequent\n",
    "    words based on their co-occurrence with more frequent words.\n",
    "\n",
    "2.Handling long sentences: SkipGram can handle long sentences because it only requires the input of a single target word at a \n",
    "    time, which makes it less sensitive to the length of the input sequence.\n",
    "\n",
    "3.Capturing syntactic relationships: SkipGram can capture syntactic relationships between words, such as subject-verb-object or\n",
    "    adjective-noun, by learning the embeddings of the context words that co-occur with the target word in different positions\n",
    "    within the sentence.\n",
    "\n",
    "SkipGram is widely used in NLP tasks such as text classification, sentiment analysis, and machine translation, where it is \n",
    "used to generate high-quality word embeddings that capture the semantic and syntactic properties of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "9.Explain Glove Embeddings.\n",
    "\n",
    "ANSWER\n",
    "\n",
    "GloVe (Global Vectors) is a type of word embedding technique in Natural Language Processing (NLP) that is based on \n",
    "matrix factorization of the word co-occurrence matrix. GloVe embeddings capture the meaning and relationships between \n",
    "words based on their co-occurrence in a large corpus of text data.\n",
    "\n",
    "In the GloVe algorithm, the word co-occurrence matrix is constructed by counting the number of times each word appears\n",
    "in the context of every other word in the corpus. The co-occurrence matrix is then factorized into two smaller matrices \n",
    "using a technique called Singular Value Decomposition (SVD).\n",
    "\n",
    "The resulting factorized matrices represent the words and their contexts in a low-dimensional space, where each word is \n",
    "represented as a dense vector of floating-point numbers. The GloVe embeddings capture both the global statistics of the \n",
    "corpus and the local context of each word, which makes them more accurate than other word embedding techniques.\n",
    "\n",
    "GloVe embeddings have several advantages over other word embedding techniques. Some of these advantages include:\n",
    "\n",
    "1.Better word analogy performance: GloVe embeddings perform better on word analogy tasks than other embedding techniques,\n",
    "    such as word2vec.\n",
    "\n",
    "2.Capturing word relationships: GloVe embeddings capture the relationships between words based on their co-occurrence patterns,\n",
    "    which makes them effective at capturing the semantic and syntactic properties of words.\n",
    "\n",
    "3.Scalability: GloVe embeddings can be trained on very large datasets because the algorithm scales linearly with the size \n",
    "    of the corpus.\n",
    "\n",
    "GloVe embeddings are widely used in NLP tasks such as text classification, sentiment analysis, machine translation,\n",
    "and information retrieval. They have proven to be an effective way to represent words and phrases in a compact and \n",
    "meaningful way that captures their semantic and syntactic properties.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
